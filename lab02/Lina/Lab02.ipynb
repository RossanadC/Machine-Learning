{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMbEf/x63fXENH9H8Fr53tt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RossanadC/Machine-Learning/blob/Lab2/Lab02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import packages**"
      ],
      "metadata": {
        "id": "5D32ttDz7AFL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OU9e9QQR6HP7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "# numpy and pytorch are already installed\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define a Neural Network**"
      ],
      "metadata": {
        "id": "lLRbvJiY69Mz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define neural network\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.lin1 = nn.Linear(28*28, 512)\n",
        "        self.act1 = nn.ReLU()\n",
        "\n",
        "        self.lin2 = nn.Linear(512, 512)\n",
        "        self.act2 = nn.ReLU()\n",
        "\n",
        "        self.output_layer = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)  # (batch_size, 28, 28) => (batch_size, 28*28)\n",
        "\n",
        "        # first layer (input is x, output is x1)\n",
        "        x1 = self.lin1(x)\n",
        "        x1 = self.act1(x1)\n",
        "\n",
        "        # second layer (input is x1, output is x2)\n",
        "        x2 = self.lin2(x1)\n",
        "        x2 = self.act2(x2)\n",
        "\n",
        "        # third/output layer (input is x2, output is logits)\n",
        "        logits = self.output_layer(x2)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "qixNWfdQ7_iC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset**"
      ],
      "metadata": {
        "id": "xZbpFFim6whO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "# preprocessing to apply on each data sample:\n",
        "# 1) convert to tensor\n",
        "# 2) normalize images\n",
        "transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.1307,), std=(0.3081,))  # Normalize a tensor image with mean and standard deviation.\n",
        "        ])\n",
        "\n",
        "# Download train data from open datasets.\n",
        "train_data= datasets.FashionMNIST(root='data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Download test data from open datsets.\n",
        "test_data=datasets.FashionMNIST(root='data', train=False, download=True, transform=transform)\n"
      ],
      "metadata": {
        "id": "S29gzGC29pkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Validation set**"
      ],
      "metadata": {
        "id": "_Uwt7-NI_PrL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, validation_data=torch.utils.data.random_split(train_data, [0.9,0.1])"
      ],
      "metadata": {
        "id": "68529ZEv_V34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plot one sample from dataset**"
      ],
      "metadata": {
        "id": "MkA0u4RRQXaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = train_data[100]"
      ],
      "metadata": {
        "id": "lf0wNTnLQcPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPRbXbORQ4Qy",
        "outputId": "026bce07-7a5b-42b2-8ff2-5eca7f4ee9d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChIRiRwYQ578",
        "outputId": "64906551-bffc-4669-c0d7-1ffe1bfd524e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset returns one sample at a time\n",
        "# In our case one sample corresponds to a Tuple: (Image, Target Label)\n",
        "# Image is a tensor with shape [1, 28, 28]\n",
        "# Label is an integer representing the class of the image\n",
        "\n",
        "print(type(train_data[100]))\n",
        "print(len(train_data[100]))\n",
        "\n",
        "image, label = train_data[100]\n",
        "print(\"image: \", image.shape, type(image))\n",
        "print(\"label: \", label, type(label))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9k9t0QUQnlU",
        "outputId": "8e96f84b-35fb-4019-fd34-1798b21ff457"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'tuple'>\n",
            "2\n",
            "image:  torch.Size([1, 28, 28]) <class 'torch.Tensor'>\n",
            "label:  6 <class 'int'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vFIT-ciQoXL",
        "outputId": "c0d5f34b-3fb0-45f7-93d4-f9c094d7a59b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6wFfeizQ-Tc",
        "outputId": "0c8b2fe2-1539-4575-8ed8-78f3e2fed087"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image=image.view(28,28)\n",
        "image.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jI0ODtgbQ_7F",
        "outputId": "7a4b9f47-4234-4209-bcfc-25f710f8d702"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot one sample normalized\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(image.numpy())  # numpy array 28x28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "Mm89lCCPRCfG",
        "outputId": "e8bdb641-5c74-4133-8ab6-55087d7764f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7e7ab558ff80>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DataLoader**"
      ],
      "metadata": {
        "id": "inwmWabfK30F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128  # we define here the batch size: number of samples processed before the model is updated\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True) #, drop_last=True)\n",
        "test_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=False) # , drop_last=False)\n",
        "print(f\"batch size: {batch_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoxKbz53NEhR",
        "outputId": "9805f117-e30c-4d4c-c4d7-21763b1a452e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch size: 128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Traning**"
      ],
      "metadata": {
        "id": "pim2UMXbO3Q5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function for training\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()  # model to train mode\n",
        "\n",
        "    # ITERATE DATALOADER: train_loader\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        #  SINGLE OPTIMIZATION STEP IS PERFORMED ON A BATCH!\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(data)\n",
        "\n",
        "        loss = F.cross_entropy(output, target)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 50 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "# test\n",
        "# function for evaluation\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()  # model to eval\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    # ITERATE DATALOADER: test_loader\n",
        "    for data, target in test_loader:\n",
        "        batch_size = data.shape[0]\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        test_loss += F.cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n",
        "\n",
        "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "\n",
        "        # sanity check\n",
        "        pred = pred.view(batch_size)  # [bs,]\n",
        "        target = target.view(batch_size)  # [bs,]\n",
        "\n",
        "        # compute prediction ok\n",
        "        batch_pred_ok = pred.eq(target).sum().item()\n",
        "        correct += batch_pred_ok\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    num_samples = len(test_loader.dataset)\n",
        "    test_accuracy = correct / num_samples\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n"
      ],
      "metadata": {
        "id": "e2bXYHtrO7OR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda')  #Â use gpu, is equivalent to .cuda()\n",
        "\n",
        "# training hyperparameters\n",
        "lr = 0.01\n",
        "num_epochs = 30\n",
        "print(f\"lr: {lr}\")\n",
        "print(f\"batch size: {batch_size}\")\n",
        "print(f\"Num. optimization steps per-epoch: {int(len(train_data)/batch_size)}\")\n",
        "\n",
        "#########\n",
        "# MODEL #\n",
        "#########\n",
        "model = NeuralNetwork(num_classes=10)\n",
        "model = model.to(device)\n",
        "\n",
        "#############\n",
        "# OPTIMIZER #\n",
        "#############\n",
        "parameters_to_optimize = model.parameters()\n",
        "optimizer = optim.SGD(parameters_to_optimize, lr=lr, momentum=0.9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MN35X9pcQb0J",
        "outputId": "1bd24750-7af3-4b59-e628-67526381c52e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lr: 0.01\n",
            "batch size: 128\n",
            "Num. optimization steps per-epoch: 421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "  train(model, device, train_loader, optimizer, epoch)\n",
        "  test(model, device, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feti3jQBQ54o",
        "outputId": "53ada20c-2880-4712-eb4b-8a29bf94a421"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/54000 (0%)]\tLoss: 2.330357\n",
            "Train Epoch: 1 [6400/54000 (12%)]\tLoss: 0.694588\n",
            "Train Epoch: 1 [12800/54000 (24%)]\tLoss: 0.589771\n",
            "Train Epoch: 1 [19200/54000 (36%)]\tLoss: 0.550117\n",
            "Train Epoch: 1 [25600/54000 (47%)]\tLoss: 0.412640\n",
            "Train Epoch: 1 [32000/54000 (59%)]\tLoss: 0.571668\n",
            "Train Epoch: 1 [38400/54000 (71%)]\tLoss: 0.376872\n",
            "Train Epoch: 1 [44800/54000 (83%)]\tLoss: 0.501848\n",
            "Train Epoch: 1 [51200/54000 (95%)]\tLoss: 0.535670\n",
            "\n",
            "Test set: Average loss: 0.4202, Accuracy: 45817/54000 (85%)\n",
            "\n",
            "Train Epoch: 2 [0/54000 (0%)]\tLoss: 0.428738\n",
            "Train Epoch: 2 [6400/54000 (12%)]\tLoss: 0.373340\n",
            "Train Epoch: 2 [12800/54000 (24%)]\tLoss: 0.308762\n",
            "Train Epoch: 2 [19200/54000 (36%)]\tLoss: 0.363187\n",
            "Train Epoch: 2 [25600/54000 (47%)]\tLoss: 0.398268\n",
            "Train Epoch: 2 [32000/54000 (59%)]\tLoss: 0.322624\n",
            "Train Epoch: 2 [38400/54000 (71%)]\tLoss: 0.332073\n",
            "Train Epoch: 2 [44800/54000 (83%)]\tLoss: 0.342679\n",
            "Train Epoch: 2 [51200/54000 (95%)]\tLoss: 0.347028\n",
            "\n",
            "Test set: Average loss: 0.3324, Accuracy: 47470/54000 (88%)\n",
            "\n",
            "Train Epoch: 3 [0/54000 (0%)]\tLoss: 0.250175\n",
            "Train Epoch: 3 [6400/54000 (12%)]\tLoss: 0.247581\n",
            "Train Epoch: 3 [12800/54000 (24%)]\tLoss: 0.407063\n",
            "Train Epoch: 3 [19200/54000 (36%)]\tLoss: 0.338158\n",
            "Train Epoch: 3 [25600/54000 (47%)]\tLoss: 0.382137\n",
            "Train Epoch: 3 [32000/54000 (59%)]\tLoss: 0.312941\n",
            "Train Epoch: 3 [38400/54000 (71%)]\tLoss: 0.373453\n",
            "Train Epoch: 3 [44800/54000 (83%)]\tLoss: 0.275868\n",
            "Train Epoch: 3 [51200/54000 (95%)]\tLoss: 0.222697\n",
            "\n",
            "Test set: Average loss: 0.3400, Accuracy: 47242/54000 (87%)\n",
            "\n",
            "Train Epoch: 4 [0/54000 (0%)]\tLoss: 0.375475\n",
            "Train Epoch: 4 [6400/54000 (12%)]\tLoss: 0.311662\n",
            "Train Epoch: 4 [12800/54000 (24%)]\tLoss: 0.255619\n",
            "Train Epoch: 4 [19200/54000 (36%)]\tLoss: 0.359624\n",
            "Train Epoch: 4 [25600/54000 (47%)]\tLoss: 0.254209\n",
            "Train Epoch: 4 [32000/54000 (59%)]\tLoss: 0.275166\n",
            "Train Epoch: 4 [38400/54000 (71%)]\tLoss: 0.389227\n",
            "Train Epoch: 4 [44800/54000 (83%)]\tLoss: 0.352562\n",
            "Train Epoch: 4 [51200/54000 (95%)]\tLoss: 0.187447\n",
            "\n",
            "Test set: Average loss: 0.2843, Accuracy: 48388/54000 (90%)\n",
            "\n",
            "Train Epoch: 5 [0/54000 (0%)]\tLoss: 0.230260\n",
            "Train Epoch: 5 [6400/54000 (12%)]\tLoss: 0.258764\n",
            "Train Epoch: 5 [12800/54000 (24%)]\tLoss: 0.269853\n",
            "Train Epoch: 5 [19200/54000 (36%)]\tLoss: 0.204718\n",
            "Train Epoch: 5 [25600/54000 (47%)]\tLoss: 0.351576\n",
            "Train Epoch: 5 [32000/54000 (59%)]\tLoss: 0.332455\n",
            "Train Epoch: 5 [38400/54000 (71%)]\tLoss: 0.191397\n",
            "Train Epoch: 5 [44800/54000 (83%)]\tLoss: 0.243778\n",
            "Train Epoch: 5 [51200/54000 (95%)]\tLoss: 0.401258\n",
            "\n",
            "Test set: Average loss: 0.2734, Accuracy: 48543/54000 (90%)\n",
            "\n",
            "Train Epoch: 6 [0/54000 (0%)]\tLoss: 0.244466\n",
            "Train Epoch: 6 [6400/54000 (12%)]\tLoss: 0.246451\n",
            "Train Epoch: 6 [12800/54000 (24%)]\tLoss: 0.295902\n",
            "Train Epoch: 6 [19200/54000 (36%)]\tLoss: 0.232541\n",
            "Train Epoch: 6 [25600/54000 (47%)]\tLoss: 0.194088\n",
            "Train Epoch: 6 [32000/54000 (59%)]\tLoss: 0.284251\n",
            "Train Epoch: 6 [38400/54000 (71%)]\tLoss: 0.163264\n",
            "Train Epoch: 6 [44800/54000 (83%)]\tLoss: 0.188174\n",
            "Train Epoch: 6 [51200/54000 (95%)]\tLoss: 0.274571\n",
            "\n",
            "Test set: Average loss: 0.2601, Accuracy: 48812/54000 (90%)\n",
            "\n",
            "Train Epoch: 7 [0/54000 (0%)]\tLoss: 0.219812\n",
            "Train Epoch: 7 [6400/54000 (12%)]\tLoss: 0.318345\n",
            "Train Epoch: 7 [12800/54000 (24%)]\tLoss: 0.277883\n",
            "Train Epoch: 7 [19200/54000 (36%)]\tLoss: 0.363377\n",
            "Train Epoch: 7 [25600/54000 (47%)]\tLoss: 0.251143\n",
            "Train Epoch: 7 [32000/54000 (59%)]\tLoss: 0.419510\n",
            "Train Epoch: 7 [38400/54000 (71%)]\tLoss: 0.245897\n",
            "Train Epoch: 7 [44800/54000 (83%)]\tLoss: 0.301104\n",
            "Train Epoch: 7 [51200/54000 (95%)]\tLoss: 0.231572\n",
            "\n",
            "Test set: Average loss: 0.2764, Accuracy: 48351/54000 (90%)\n",
            "\n",
            "Train Epoch: 8 [0/54000 (0%)]\tLoss: 0.350596\n",
            "Train Epoch: 8 [6400/54000 (12%)]\tLoss: 0.270795\n",
            "Train Epoch: 8 [12800/54000 (24%)]\tLoss: 0.271564\n",
            "Train Epoch: 8 [19200/54000 (36%)]\tLoss: 0.274400\n",
            "Train Epoch: 8 [25600/54000 (47%)]\tLoss: 0.207702\n",
            "Train Epoch: 8 [32000/54000 (59%)]\tLoss: 0.289914\n",
            "Train Epoch: 8 [38400/54000 (71%)]\tLoss: 0.269151\n",
            "Train Epoch: 8 [44800/54000 (83%)]\tLoss: 0.317438\n",
            "Train Epoch: 8 [51200/54000 (95%)]\tLoss: 0.344515\n",
            "\n",
            "Test set: Average loss: 0.2254, Accuracy: 49430/54000 (92%)\n",
            "\n",
            "Train Epoch: 9 [0/54000 (0%)]\tLoss: 0.177992\n",
            "Train Epoch: 9 [6400/54000 (12%)]\tLoss: 0.182887\n",
            "Train Epoch: 9 [12800/54000 (24%)]\tLoss: 0.213102\n",
            "Train Epoch: 9 [19200/54000 (36%)]\tLoss: 0.235108\n",
            "Train Epoch: 9 [25600/54000 (47%)]\tLoss: 0.320926\n",
            "Train Epoch: 9 [32000/54000 (59%)]\tLoss: 0.222811\n",
            "Train Epoch: 9 [38400/54000 (71%)]\tLoss: 0.235080\n",
            "Train Epoch: 9 [44800/54000 (83%)]\tLoss: 0.214015\n",
            "Train Epoch: 9 [51200/54000 (95%)]\tLoss: 0.231438\n",
            "\n",
            "Test set: Average loss: 0.2144, Accuracy: 49780/54000 (92%)\n",
            "\n",
            "Train Epoch: 10 [0/54000 (0%)]\tLoss: 0.209469\n",
            "Train Epoch: 10 [6400/54000 (12%)]\tLoss: 0.221332\n",
            "Train Epoch: 10 [12800/54000 (24%)]\tLoss: 0.306157\n",
            "Train Epoch: 10 [19200/54000 (36%)]\tLoss: 0.175261\n",
            "Train Epoch: 10 [25600/54000 (47%)]\tLoss: 0.193172\n",
            "Train Epoch: 10 [32000/54000 (59%)]\tLoss: 0.165312\n",
            "Train Epoch: 10 [38400/54000 (71%)]\tLoss: 0.209243\n",
            "Train Epoch: 10 [44800/54000 (83%)]\tLoss: 0.273519\n",
            "Train Epoch: 10 [51200/54000 (95%)]\tLoss: 0.304872\n",
            "\n",
            "Test set: Average loss: 0.2037, Accuracy: 49946/54000 (92%)\n",
            "\n",
            "Train Epoch: 11 [0/54000 (0%)]\tLoss: 0.216959\n",
            "Train Epoch: 11 [6400/54000 (12%)]\tLoss: 0.195712\n",
            "Train Epoch: 11 [12800/54000 (24%)]\tLoss: 0.210159\n",
            "Train Epoch: 11 [19200/54000 (36%)]\tLoss: 0.260325\n",
            "Train Epoch: 11 [25600/54000 (47%)]\tLoss: 0.233906\n",
            "Train Epoch: 11 [32000/54000 (59%)]\tLoss: 0.216139\n",
            "Train Epoch: 11 [38400/54000 (71%)]\tLoss: 0.179798\n",
            "Train Epoch: 11 [44800/54000 (83%)]\tLoss: 0.164953\n",
            "Train Epoch: 11 [51200/54000 (95%)]\tLoss: 0.208538\n",
            "\n",
            "Test set: Average loss: 0.2060, Accuracy: 49917/54000 (92%)\n",
            "\n",
            "Train Epoch: 12 [0/54000 (0%)]\tLoss: 0.222671\n",
            "Train Epoch: 12 [6400/54000 (12%)]\tLoss: 0.331035\n",
            "Train Epoch: 12 [12800/54000 (24%)]\tLoss: 0.200556\n",
            "Train Epoch: 12 [19200/54000 (36%)]\tLoss: 0.189951\n",
            "Train Epoch: 12 [25600/54000 (47%)]\tLoss: 0.112799\n",
            "Train Epoch: 12 [32000/54000 (59%)]\tLoss: 0.223687\n",
            "Train Epoch: 12 [38400/54000 (71%)]\tLoss: 0.230163\n",
            "Train Epoch: 12 [44800/54000 (83%)]\tLoss: 0.213556\n",
            "Train Epoch: 12 [51200/54000 (95%)]\tLoss: 0.196760\n",
            "\n",
            "Test set: Average loss: 0.1946, Accuracy: 50030/54000 (93%)\n",
            "\n",
            "Train Epoch: 13 [0/54000 (0%)]\tLoss: 0.263458\n",
            "Train Epoch: 13 [6400/54000 (12%)]\tLoss: 0.259638\n",
            "Train Epoch: 13 [12800/54000 (24%)]\tLoss: 0.205235\n",
            "Train Epoch: 13 [19200/54000 (36%)]\tLoss: 0.166911\n",
            "Train Epoch: 13 [25600/54000 (47%)]\tLoss: 0.229095\n",
            "Train Epoch: 13 [32000/54000 (59%)]\tLoss: 0.156207\n",
            "Train Epoch: 13 [38400/54000 (71%)]\tLoss: 0.126352\n",
            "Train Epoch: 13 [44800/54000 (83%)]\tLoss: 0.258309\n",
            "Train Epoch: 13 [51200/54000 (95%)]\tLoss: 0.229987\n",
            "\n",
            "Test set: Average loss: 0.1788, Accuracy: 50574/54000 (94%)\n",
            "\n",
            "Train Epoch: 14 [0/54000 (0%)]\tLoss: 0.205941\n",
            "Train Epoch: 14 [6400/54000 (12%)]\tLoss: 0.186882\n",
            "Train Epoch: 14 [12800/54000 (24%)]\tLoss: 0.256473\n",
            "Train Epoch: 14 [19200/54000 (36%)]\tLoss: 0.140015\n",
            "Train Epoch: 14 [25600/54000 (47%)]\tLoss: 0.169640\n",
            "Train Epoch: 14 [32000/54000 (59%)]\tLoss: 0.154714\n",
            "Train Epoch: 14 [38400/54000 (71%)]\tLoss: 0.137241\n",
            "Train Epoch: 14 [44800/54000 (83%)]\tLoss: 0.203716\n",
            "Train Epoch: 14 [51200/54000 (95%)]\tLoss: 0.161262\n",
            "\n",
            "Test set: Average loss: 0.1807, Accuracy: 50456/54000 (93%)\n",
            "\n",
            "Train Epoch: 15 [0/54000 (0%)]\tLoss: 0.179086\n",
            "Train Epoch: 15 [6400/54000 (12%)]\tLoss: 0.200848\n",
            "Train Epoch: 15 [12800/54000 (24%)]\tLoss: 0.152046\n",
            "Train Epoch: 15 [19200/54000 (36%)]\tLoss: 0.209334\n",
            "Train Epoch: 15 [25600/54000 (47%)]\tLoss: 0.127367\n",
            "Train Epoch: 15 [32000/54000 (59%)]\tLoss: 0.178775\n",
            "Train Epoch: 15 [38400/54000 (71%)]\tLoss: 0.246280\n",
            "Train Epoch: 15 [44800/54000 (83%)]\tLoss: 0.195740\n",
            "Train Epoch: 15 [51200/54000 (95%)]\tLoss: 0.219152\n",
            "\n",
            "Test set: Average loss: 0.1598, Accuracy: 50884/54000 (94%)\n",
            "\n",
            "Train Epoch: 16 [0/54000 (0%)]\tLoss: 0.186603\n",
            "Train Epoch: 16 [6400/54000 (12%)]\tLoss: 0.166598\n",
            "Train Epoch: 16 [12800/54000 (24%)]\tLoss: 0.125340\n",
            "Train Epoch: 16 [19200/54000 (36%)]\tLoss: 0.192000\n",
            "Train Epoch: 16 [25600/54000 (47%)]\tLoss: 0.150826\n",
            "Train Epoch: 16 [32000/54000 (59%)]\tLoss: 0.135784\n",
            "Train Epoch: 16 [38400/54000 (71%)]\tLoss: 0.227848\n",
            "Train Epoch: 16 [44800/54000 (83%)]\tLoss: 0.151982\n",
            "Train Epoch: 16 [51200/54000 (95%)]\tLoss: 0.110249\n",
            "\n",
            "Test set: Average loss: 0.1681, Accuracy: 50584/54000 (94%)\n",
            "\n",
            "Train Epoch: 17 [0/54000 (0%)]\tLoss: 0.166332\n",
            "Train Epoch: 17 [6400/54000 (12%)]\tLoss: 0.121051\n",
            "Train Epoch: 17 [12800/54000 (24%)]\tLoss: 0.071035\n",
            "Train Epoch: 17 [19200/54000 (36%)]\tLoss: 0.142499\n",
            "Train Epoch: 17 [25600/54000 (47%)]\tLoss: 0.158056\n",
            "Train Epoch: 17 [32000/54000 (59%)]\tLoss: 0.225879\n",
            "Train Epoch: 17 [38400/54000 (71%)]\tLoss: 0.185459\n",
            "Train Epoch: 17 [44800/54000 (83%)]\tLoss: 0.173888\n",
            "Train Epoch: 17 [51200/54000 (95%)]\tLoss: 0.131322\n",
            "\n",
            "Test set: Average loss: 0.1488, Accuracy: 50979/54000 (94%)\n",
            "\n",
            "Train Epoch: 18 [0/54000 (0%)]\tLoss: 0.149080\n",
            "Train Epoch: 18 [6400/54000 (12%)]\tLoss: 0.083971\n",
            "Train Epoch: 18 [12800/54000 (24%)]\tLoss: 0.146928\n",
            "Train Epoch: 18 [19200/54000 (36%)]\tLoss: 0.239973\n",
            "Train Epoch: 18 [25600/54000 (47%)]\tLoss: 0.212383\n",
            "Train Epoch: 18 [32000/54000 (59%)]\tLoss: 0.153725\n",
            "Train Epoch: 18 [38400/54000 (71%)]\tLoss: 0.180276\n",
            "Train Epoch: 18 [44800/54000 (83%)]\tLoss: 0.164359\n",
            "Train Epoch: 18 [51200/54000 (95%)]\tLoss: 0.169052\n",
            "\n",
            "Test set: Average loss: 0.1456, Accuracy: 51073/54000 (95%)\n",
            "\n",
            "Train Epoch: 19 [0/54000 (0%)]\tLoss: 0.176389\n",
            "Train Epoch: 19 [6400/54000 (12%)]\tLoss: 0.111450\n",
            "Train Epoch: 19 [12800/54000 (24%)]\tLoss: 0.149633\n",
            "Train Epoch: 19 [19200/54000 (36%)]\tLoss: 0.190673\n",
            "Train Epoch: 19 [25600/54000 (47%)]\tLoss: 0.129138\n",
            "Train Epoch: 19 [32000/54000 (59%)]\tLoss: 0.097767\n",
            "Train Epoch: 19 [38400/54000 (71%)]\tLoss: 0.103723\n",
            "Train Epoch: 19 [44800/54000 (83%)]\tLoss: 0.092701\n",
            "Train Epoch: 19 [51200/54000 (95%)]\tLoss: 0.130805\n",
            "\n",
            "Test set: Average loss: 0.1608, Accuracy: 50757/54000 (94%)\n",
            "\n",
            "Train Epoch: 20 [0/54000 (0%)]\tLoss: 0.202890\n",
            "Train Epoch: 20 [6400/54000 (12%)]\tLoss: 0.122024\n",
            "Train Epoch: 20 [12800/54000 (24%)]\tLoss: 0.156605\n",
            "Train Epoch: 20 [19200/54000 (36%)]\tLoss: 0.156310\n",
            "Train Epoch: 20 [25600/54000 (47%)]\tLoss: 0.158524\n",
            "Train Epoch: 20 [32000/54000 (59%)]\tLoss: 0.160340\n",
            "Train Epoch: 20 [38400/54000 (71%)]\tLoss: 0.136625\n",
            "Train Epoch: 20 [44800/54000 (83%)]\tLoss: 0.106094\n",
            "Train Epoch: 20 [51200/54000 (95%)]\tLoss: 0.118369\n",
            "\n",
            "Test set: Average loss: 0.1342, Accuracy: 51342/54000 (95%)\n",
            "\n",
            "Train Epoch: 21 [0/54000 (0%)]\tLoss: 0.090312\n",
            "Train Epoch: 21 [6400/54000 (12%)]\tLoss: 0.121884\n",
            "Train Epoch: 21 [12800/54000 (24%)]\tLoss: 0.232992\n",
            "Train Epoch: 21 [19200/54000 (36%)]\tLoss: 0.161395\n",
            "Train Epoch: 21 [25600/54000 (47%)]\tLoss: 0.108881\n",
            "Train Epoch: 21 [32000/54000 (59%)]\tLoss: 0.169744\n",
            "Train Epoch: 21 [38400/54000 (71%)]\tLoss: 0.184726\n",
            "Train Epoch: 21 [44800/54000 (83%)]\tLoss: 0.131280\n",
            "Train Epoch: 21 [51200/54000 (95%)]\tLoss: 0.092517\n",
            "\n",
            "Test set: Average loss: 0.1167, Accuracy: 51674/54000 (96%)\n",
            "\n",
            "Train Epoch: 22 [0/54000 (0%)]\tLoss: 0.123522\n",
            "Train Epoch: 22 [6400/54000 (12%)]\tLoss: 0.149385\n",
            "Train Epoch: 22 [12800/54000 (24%)]\tLoss: 0.193604\n",
            "Train Epoch: 22 [19200/54000 (36%)]\tLoss: 0.171965\n",
            "Train Epoch: 22 [25600/54000 (47%)]\tLoss: 0.180221\n",
            "Train Epoch: 22 [32000/54000 (59%)]\tLoss: 0.121928\n",
            "Train Epoch: 22 [38400/54000 (71%)]\tLoss: 0.121510\n",
            "Train Epoch: 22 [44800/54000 (83%)]\tLoss: 0.105621\n",
            "Train Epoch: 22 [51200/54000 (95%)]\tLoss: 0.082106\n",
            "\n",
            "Test set: Average loss: 0.1259, Accuracy: 51476/54000 (95%)\n",
            "\n",
            "Train Epoch: 23 [0/54000 (0%)]\tLoss: 0.077281\n",
            "Train Epoch: 23 [6400/54000 (12%)]\tLoss: 0.092343\n",
            "Train Epoch: 23 [12800/54000 (24%)]\tLoss: 0.092947\n",
            "Train Epoch: 23 [19200/54000 (36%)]\tLoss: 0.081042\n",
            "Train Epoch: 23 [25600/54000 (47%)]\tLoss: 0.106232\n",
            "Train Epoch: 23 [32000/54000 (59%)]\tLoss: 0.145769\n",
            "Train Epoch: 23 [38400/54000 (71%)]\tLoss: 0.217626\n",
            "Train Epoch: 23 [44800/54000 (83%)]\tLoss: 0.110755\n",
            "Train Epoch: 23 [51200/54000 (95%)]\tLoss: 0.148269\n",
            "\n",
            "Test set: Average loss: 0.1136, Accuracy: 51771/54000 (96%)\n",
            "\n",
            "Train Epoch: 24 [0/54000 (0%)]\tLoss: 0.117287\n",
            "Train Epoch: 24 [6400/54000 (12%)]\tLoss: 0.170044\n",
            "Train Epoch: 24 [12800/54000 (24%)]\tLoss: 0.242508\n",
            "Train Epoch: 24 [19200/54000 (36%)]\tLoss: 0.191356\n",
            "Train Epoch: 24 [25600/54000 (47%)]\tLoss: 0.077333\n",
            "Train Epoch: 24 [32000/54000 (59%)]\tLoss: 0.077996\n",
            "Train Epoch: 24 [38400/54000 (71%)]\tLoss: 0.120543\n",
            "Train Epoch: 24 [44800/54000 (83%)]\tLoss: 0.088355\n",
            "Train Epoch: 24 [51200/54000 (95%)]\tLoss: 0.102587\n",
            "\n",
            "Test set: Average loss: 0.1285, Accuracy: 51390/54000 (95%)\n",
            "\n",
            "Train Epoch: 25 [0/54000 (0%)]\tLoss: 0.100649\n",
            "Train Epoch: 25 [6400/54000 (12%)]\tLoss: 0.176186\n",
            "Train Epoch: 25 [12800/54000 (24%)]\tLoss: 0.150067\n",
            "Train Epoch: 25 [19200/54000 (36%)]\tLoss: 0.065785\n",
            "Train Epoch: 25 [25600/54000 (47%)]\tLoss: 0.104185\n",
            "Train Epoch: 25 [32000/54000 (59%)]\tLoss: 0.120608\n",
            "Train Epoch: 25 [38400/54000 (71%)]\tLoss: 0.172584\n",
            "Train Epoch: 25 [44800/54000 (83%)]\tLoss: 0.197441\n",
            "Train Epoch: 25 [51200/54000 (95%)]\tLoss: 0.230665\n",
            "\n",
            "Test set: Average loss: 0.1011, Accuracy: 51979/54000 (96%)\n",
            "\n",
            "Train Epoch: 26 [0/54000 (0%)]\tLoss: 0.138716\n",
            "Train Epoch: 26 [6400/54000 (12%)]\tLoss: 0.137064\n",
            "Train Epoch: 26 [12800/54000 (24%)]\tLoss: 0.103309\n",
            "Train Epoch: 26 [19200/54000 (36%)]\tLoss: 0.093237\n",
            "Train Epoch: 26 [25600/54000 (47%)]\tLoss: 0.119250\n",
            "Train Epoch: 26 [32000/54000 (59%)]\tLoss: 0.133057\n",
            "Train Epoch: 26 [38400/54000 (71%)]\tLoss: 0.151549\n",
            "Train Epoch: 26 [44800/54000 (83%)]\tLoss: 0.149424\n",
            "Train Epoch: 26 [51200/54000 (95%)]\tLoss: 0.119857\n",
            "\n",
            "Test set: Average loss: 0.0930, Accuracy: 52173/54000 (97%)\n",
            "\n",
            "Train Epoch: 27 [0/54000 (0%)]\tLoss: 0.140317\n",
            "Train Epoch: 27 [6400/54000 (12%)]\tLoss: 0.092843\n",
            "Train Epoch: 27 [12800/54000 (24%)]\tLoss: 0.052631\n",
            "Train Epoch: 27 [19200/54000 (36%)]\tLoss: 0.068977\n",
            "Train Epoch: 27 [25600/54000 (47%)]\tLoss: 0.056220\n",
            "Train Epoch: 27 [32000/54000 (59%)]\tLoss: 0.105717\n",
            "Train Epoch: 27 [38400/54000 (71%)]\tLoss: 0.068422\n",
            "Train Epoch: 27 [44800/54000 (83%)]\tLoss: 0.090941\n",
            "Train Epoch: 27 [51200/54000 (95%)]\tLoss: 0.117663\n",
            "\n",
            "Test set: Average loss: 0.1075, Accuracy: 51855/54000 (96%)\n",
            "\n",
            "Train Epoch: 28 [0/54000 (0%)]\tLoss: 0.059791\n",
            "Train Epoch: 28 [6400/54000 (12%)]\tLoss: 0.070767\n",
            "Train Epoch: 28 [12800/54000 (24%)]\tLoss: 0.115689\n",
            "Train Epoch: 28 [19200/54000 (36%)]\tLoss: 0.106082\n",
            "Train Epoch: 28 [25600/54000 (47%)]\tLoss: 0.080598\n",
            "Train Epoch: 28 [32000/54000 (59%)]\tLoss: 0.119034\n",
            "Train Epoch: 28 [38400/54000 (71%)]\tLoss: 0.050240\n",
            "Train Epoch: 28 [44800/54000 (83%)]\tLoss: 0.146258\n",
            "Train Epoch: 28 [51200/54000 (95%)]\tLoss: 0.081649\n",
            "\n",
            "Test set: Average loss: 0.0928, Accuracy: 52189/54000 (97%)\n",
            "\n",
            "Train Epoch: 29 [0/54000 (0%)]\tLoss: 0.089730\n",
            "Train Epoch: 29 [6400/54000 (12%)]\tLoss: 0.075310\n",
            "Train Epoch: 29 [12800/54000 (24%)]\tLoss: 0.101208\n",
            "Train Epoch: 29 [19200/54000 (36%)]\tLoss: 0.174348\n",
            "Train Epoch: 29 [25600/54000 (47%)]\tLoss: 0.109074\n",
            "Train Epoch: 29 [32000/54000 (59%)]\tLoss: 0.062013\n",
            "Train Epoch: 29 [38400/54000 (71%)]\tLoss: 0.122764\n",
            "Train Epoch: 29 [44800/54000 (83%)]\tLoss: 0.083437\n",
            "Train Epoch: 29 [51200/54000 (95%)]\tLoss: 0.053280\n",
            "\n",
            "Test set: Average loss: 0.0965, Accuracy: 51993/54000 (96%)\n",
            "\n",
            "Train Epoch: 30 [0/54000 (0%)]\tLoss: 0.093102\n",
            "Train Epoch: 30 [6400/54000 (12%)]\tLoss: 0.074305\n",
            "Train Epoch: 30 [12800/54000 (24%)]\tLoss: 0.056643\n",
            "Train Epoch: 30 [19200/54000 (36%)]\tLoss: 0.085375\n",
            "Train Epoch: 30 [25600/54000 (47%)]\tLoss: 0.070466\n",
            "Train Epoch: 30 [32000/54000 (59%)]\tLoss: 0.088746\n",
            "Train Epoch: 30 [38400/54000 (71%)]\tLoss: 0.054144\n",
            "Train Epoch: 30 [44800/54000 (83%)]\tLoss: 0.065900\n",
            "Train Epoch: 30 [51200/54000 (95%)]\tLoss: 0.136552\n",
            "\n",
            "Test set: Average loss: 0.0746, Accuracy: 52565/54000 (97%)\n",
            "\n",
            "CPU times: user 10min 44s, sys: 1.23 s, total: 10min 45s\n",
            "Wall time: 10min 51s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4oIlaMnAQ5oU"
      }
    }
  ]
}
